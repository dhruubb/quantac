# """
# build_vectorstore.py
# Builds a FAISS vector store from MD&A PDFs.
# Currently scoped to ICICI Bank FY2024-25 for testing.

# To add more companies/years later:
#   - Uncomment entries in COMPANIES / YEARS
#   - Place PDFs at data/<company_dir>/<pdf_filename>
#   - Re-run this script
# """

# import os
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS

# from ingest_mda import extract_mda_sections, chunk_sections


# DATA_DIR = "data"
# VECTORSTORE_DIR = "vectorstore/mda_faiss"

# # â”€â”€ Companies to index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# COMPANIES = {
#     "icici": "ICICI Bank",
#     # "tcs":        "TCS",
#     # "infosys":    "Infosys",
#     # "reliance":   "Reliance Industries",
#     # "adani_power":"Adani Power",
# }

# # â”€â”€ PDFs to include per company â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # Place files at:  data/<company_dir>/<pdf_filename>
# YEARS = {
#     "mda_2425.pdf": "FY2024-25",
#     # "mda_2324.pdf": "FY2023-24",
# }


# def build_vectorstore():
#     print("ğŸš€ Starting FAISS vectorstore build...\n")

#     embeddings = HuggingFaceEmbeddings(
#         model_name="sentence-transformers/all-MiniLM-L6-v2"
#     )

#     vectorstore = None
#     total_chunks = 0

#     for company_dir, company_name in COMPANIES.items():
#         company_path = os.path.join(DATA_DIR, company_dir)

#         for pdf_filename, year in YEARS.items():
#             pdf_path = os.path.join(company_path, pdf_filename)

#             if not os.path.exists(pdf_path):
#                 print(f"âš ï¸  Not found â€” skipping: {pdf_path}")
#                 continue

#             print(f"ğŸ“„ Processing : {company_name} | {year}")
#             print(f"   Path       : {pdf_path}")

#             sections = extract_mda_sections(pdf_path)
#             chunks = chunk_sections(sections, company_name, year)

#             if not chunks:
#                 print(f"   âš ï¸  No chunks extracted â€” check PDF text layer")
#                 continue

#             texts = [c["content"] for c in chunks]
#             metadatas = [c["metadata"] for c in chunks]

#             print(f"   âœ… {len(chunks)} chunks ready for indexing")
#             total_chunks += len(chunks)

#             if vectorstore is None:
#                 vectorstore = FAISS.from_texts(
#                     texts=texts,
#                     embedding=embeddings,
#                     metadatas=metadatas
#                 )
#             else:
#                 vectorstore.add_texts(texts=texts, metadatas=metadatas)

#     if vectorstore is None:
#         print("\nâŒ No documents were processed.")
#         print("   Make sure your PDF is at: data/icici/mda_2425.pdf")
#         return

#     os.makedirs(VECTORSTORE_DIR, exist_ok=True)
#     vectorstore.save_local(VECTORSTORE_DIR)

#     print(f"\nâœ… Vectorstore saved to : {VECTORSTORE_DIR}")
#     print(f"ğŸ“Š Total chunks indexed : {total_chunks}")


# if __name__ == "__main__":
#     build_vectorstore()

"""
build_vectorstore.py
Builds a FAISS vector store from all company MD&A PDFs + Excel financial data.
"""

import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from ingest_mda import extract_mda_sections, chunk_sections
from ingest_excel import ingest_excel


VECTORSTORE_DIR = "vectorstore/mda_faiss"

# â”€â”€ MD&A PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (folder, filename, company_name, year)
PDF_DOCUMENTS = [
    # ICICI Bank
    ("icici", "icici-bank-ar-2024-managements-discussion-and-analysis.pdf", "ICICI Bank", "FY2023-24"),
    ("icici", "icici-bank-ar-2025-managements-discussion-and-analysis.pdf", "ICICI Bank", "FY2024-25"),

    # TCS
    ("tcs", "TCS MD&A 2024.pdf", "TCS", "FY2023-24"),
    ("tcs", "TCS MD&A 2025.pdf", "TCS", "FY2024-25"),

    # Infosys
    ("infosys", "INFOSYS MD&A 2024.pdf", "Infosys", "FY2023-24"),
    ("infosys", "Infosys MD&A 2025.pdf", "Infosys", "FY2024-25"),

    # Reliance
    ("reliance", "Reliance MD&A 2024.pdf", "Reliance Industries", "FY2023-24"),
    ("reliance", "Reliance MD&A 2025.pdf", "Reliance Industries", "FY2024-25"),

    # Adani Power â€” note the capitalisation difference in filenames
    ("adani power", "Adani Power MD&A 2024.pdf", "Adani Power", "FY2023-24"),
    ("adani power", "Adani power MD&A 2025.pdf", "Adani Power", "FY2024-25"),
]

# â”€â”€ Excel Financial Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (folder, filename, company_name)
EXCEL_DOCUMENTS = [
    ("icici",       "ICICI Bank.xlsx",          "ICICI Bank"),
    ("tcs",         "TCS.xlsx",                 "TCS"),
    ("infosys",     "Infosys.xlsx",             "Infosys"),
    ("reliance",    "Reliance Industr.xlsx",    "Reliance Industries"),
    ("adani power", "Adani Power.xlsx",         "Adani Power"),
]


def build_vectorstore():
    print("ğŸš€ Starting FAISS vectorstore build...\n")

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    vectorstore = None
    total_chunks = 0
    pdf_processed = 0
    excel_processed = 0
    skipped = 0

    # â”€â”€ Step 1: Ingest MD&A PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("=" * 55)
    print("ğŸ“„ INGESTING MD&A PDFs")
    print("=" * 55)

    for folder, filename, company_name, year in PDF_DOCUMENTS:
        pdf_path = os.path.join(folder, filename)

        if not os.path.exists(pdf_path):
            print(f"âš ï¸  Not found â€” skipping : {pdf_path}")
            skipped += 1
            continue

        print(f"\nğŸ“„ {company_name} | {year}")
        print(f"   {pdf_path}")

        try:
            sections = extract_mda_sections(pdf_path)
            chunks = chunk_sections(sections, company_name, year)
        except Exception as e:
            print(f"   âŒ Error: {e}")
            skipped += 1
            continue

        if not chunks:
            print(f"   âš ï¸  No chunks extracted")
            skipped += 1
            continue

        texts = [c["content"] for c in chunks]
        metadatas = [c["metadata"] for c in chunks]
        total_chunks += len(chunks)
        pdf_processed += 1
        print(f"   âœ… {len(chunks)} chunks")

        if vectorstore is None:
            vectorstore = FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas)
        else:
            vectorstore.add_texts(texts=texts, metadatas=metadatas)

    # â”€â”€ Step 2: Ingest Excel Financial Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*55}")
    print("ğŸ“Š INGESTING EXCEL FINANCIAL DATA")
    print("=" * 55)

    for folder, filename, company_name in EXCEL_DOCUMENTS:
        excel_path = os.path.join(folder, filename)

        if not os.path.exists(excel_path):
            print(f"âš ï¸  Not found â€” skipping : {excel_path}")
            skipped += 1
            continue

        print(f"\nğŸ“Š {company_name} â€” {excel_path}")

        try:
            chunks = ingest_excel(excel_path, company_name)
        except Exception as e:
            print(f"   âŒ Error: {e}")
            skipped += 1
            continue

        if not chunks:
            print(f"   âš ï¸  No chunks extracted")
            skipped += 1
            continue

        texts = [c["content"] for c in chunks]
        metadatas = [c["metadata"] for c in chunks]
        total_chunks += len(chunks)
        excel_processed += 1
        print(f"   âœ… {len(chunks)} chunks ({', '.join(set(m['section'] for m in metadatas))})")

        if vectorstore is None:
            vectorstore = FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas)
        else:
            vectorstore.add_texts(texts=texts, metadatas=metadatas)

    # â”€â”€ Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if vectorstore is None:
        print("\nâŒ No documents processed. Check file paths.")
        return

    os.makedirs(VECTORSTORE_DIR, exist_ok=True)
    vectorstore.save_local(VECTORSTORE_DIR)

    print(f"\n{'='*55}")
    print(f"âœ… Vectorstore saved to : {VECTORSTORE_DIR}")
    print(f"ğŸ“Š Total chunks indexed : {total_chunks}")
    print(f"ğŸ“„ PDFs processed       : {pdf_processed}")
    print(f"ğŸ“Š Excel files processed: {excel_processed}")
    print(f"âš ï¸  Files skipped        : {skipped}")
    print(f"{'='*55}")


if __name__ == "__main__":
    build_vectorstore()